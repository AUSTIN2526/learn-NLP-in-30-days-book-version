# Ch.06 萬物皆可Transformer
章節難度: ★★★★☆
## 內容簡介
不論是圖片、影片、音訊、還是文字，只要是學習人工智慧的人，一定會知道Transformer 架構，這個模型與Seq2Seq 同樣採用了Encoder-Decoder 架構，但Transformer 的優勢在於運算速度更快，並且能應用於多個不同領域。在本章中，我們將初步了解Transformer 的實際應用範圍。

1. #### 自注意力機制與位置編碼：深入探討自注意力機制（Self-Attention）的工作原理，說明其如何在無須考慮序列順序的情況下計算序列元素間的關聯性，以及位置編碼（Positional Encoding ）如何為模型提供序列位置訊息，使其能夠更好地處理序列資料。
2. #### Transformer Encoder與Decoder運算方式：詳細解析Transformer架構中Encoder與Decoder 的運算方式，介紹其層級結構、主要組成部分的工作流程，並說明它們在處理語言模型時的具體應用。
3. #### 文字摘要模型實作：在本章中會展示如何建立Transformer模型，以及如何透過遮罩矩陣來隱藏未來的訊息和填補的文字位置。

## 額外教材推薦
* 無

## 修改紀錄
* 無
