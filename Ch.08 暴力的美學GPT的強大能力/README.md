# Ch.08 暴力的美學GPT的強大能力
章節難度: ★★★☆☆
## 內容簡介
如果將BERT 視為Transformer 架構中Encoder 的代表，那麼GPT 則可視為Decoder 的典範，但是單純Decoder 的模型相較於只使用Encoder 的模型，理解語義內容，然而GPT 的設計中，採用了一種簡單粗暴但有效的策略來解決這些問題，就是「擴大模型規模」和「豐富訓練資料」，這也是GPT 系列在訓練過程中經常使用的作法，因此本章將重點介紹GPT 家族在訓練過程中採用的方法。

1. #### 理解GPT系列模型的訓練方式：GPT系列的模型往往都是採用無監督學習，這一點使其在進行訓練時不需要有標籤即可，而GPT 系列的模型在此基礎上進行了一定的優化，使其成為通用的模型。
2. #### 理解少樣本與模型之間的關係：少樣本（Few-Shot）是一個在大型語言模型中的重要概念，其能夠幫助模型在未經過微調的情況下，進行更完善的推理，但是少樣本的效果需要模型的參數量夠大才能實現。
3. #### QA問答實作練習：在上一章節中，我們使用BERT做QA問答，這裡我們會使用生成的方式來進行QA 問答，這時損失值的遮罩就需要有良好的設計與考量。

## 額外教材推薦
* 無

## 修改紀錄
* 無
