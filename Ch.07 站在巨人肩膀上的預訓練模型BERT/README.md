# Ch.07 站在巨人肩膀上的預訓練模型BERT
章節難度: ★★☆☆☆
## 內容簡介
在上一章中，訓練Transformer 花費了非常多的時間。如果我們遇到新的任務，就需要重新訓練模型，這不僅消耗大量電力資源，還需要訓練多個模型，因此最好的解決方式就是使用別人訓練好的模型權重來幫助我們，這就是「預訓練模型」的主要目的，其方式是利用訓練好的模型權重，讓我們能快速根據自己的資料進行調整。在本章中，我將介紹BERT 這一個預訓練模型及其應用。

1. #### 初步理解預訓練模型：「預訓練模型」是機器學習中的一種策略，先利用大量資料進行初步訓練，再針對特定任務進行微調，這種方法能提高模型在多種任務中的表現。
2. #### BERT的預訓練策略：BERT的兩種主要預訓練策略─「遮罩語言模型」（MLM）和「下一句預測」（NSP），在預訓練模型中具有強大的功能，BERT 也是依靠這些方法，成為當時最強大的模型之一。
3. #### QA問答實作練習：QA問答通常由生成式語言模型來完成，但BERT是一個Encoder 架構，因此只能進行分類。我們需要學習如何利用Encoder 模型來實現QA 問答功能。

## 額外教材推薦
* 無

## 修改紀錄
* 無
