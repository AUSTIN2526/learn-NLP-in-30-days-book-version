{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 空白斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Hello', 'I', 'Python', 'You', 'a', 'am', 'are', 'human', 'language', 'like', 'love', 'natural', 'processing', 'robot']\n"
     ]
    }
   ],
   "source": [
    "# 模擬文本資料\n",
    "english_sentence = [\n",
    "    'I love natural language processing',\n",
    "    'Hello Python',\n",
    "    'I like Apple',\n",
    "    'I am a human',\n",
    "    'You are a robot',\n",
    "]\n",
    "\n",
    "vocab = [] # 分析文本後產生的詞彙表\n",
    "for sentence in english_sentence:\n",
    "\ttokens = sentence.split(' ') # 空白斷詞產生token\n",
    "\tvocab.extend(tokens) \n",
    "\t\n",
    "vocab = sorted(set(vocab)) # 通過set()過濾重複單字，並用sorted()進行排序\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        vocab = ['<UNK>'] + vocab # 讓不存在詞彙表的token能夠轉換成<UNK>\n",
    "        self.tokens_to_ids = {token:idx for idx, token in enumerate(vocab)}  # 初始化對應數字的對應表\n",
    "    \n",
    "    def __call__(self, sentence):\n",
    "        words = sentence.split()\n",
    "        unk_token_ids = self.tokens_to_ids['<UNK>']\n",
    "        return [self.tokens_to_ids.get(word, unk_token_ids) for word in words]\n",
    "        \n",
    "    \n",
    "tokenizer = Tokenizer(vocab) # 初始化類別\n",
    "input_ids = tokenizer('processing & process') # 使用tokenizer\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "當前的Token: {'<PAD>', '<UNK>'}\n",
      "當前的詞彙表: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('n', 'e', 'w', 'e', 's', 't'): 6, ('w', 'i', 'd', 'e', 's', 't'): 3}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, vocab, pad_token='<PAD>', unk_token='<UNK>'):\n",
    "        self.pad_token = pad_token # 填充字元的Token\n",
    "        self.unk_token = unk_token # 未知字元的Token\n",
    "        self.vocab = {tuple(word): freq for word, freq in vocab.items()}  # 建立詞彙表\n",
    "        self.tokens = set([pad_token, unk_token]) # 用於儲存被BEP分割出來的Token\n",
    "        self.token_to_id = {pad_token: 0, unk_token: 1}  # 文字轉數字\n",
    "        self.id_to_token = {0: pad_token, 1: unk_token}  # 數字轉文字\n",
    "\n",
    "    def get_stats(self):\n",
    "        pairs = Counter()\n",
    "        for word, freq in self.vocab.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                pairs[word[i], word[i + 1]] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair):\n",
    "        new_token = ''.join(pair) # 將頻率最高的字元對轉成字串\n",
    "\n",
    "        # 更新相關資料\n",
    "        if new_token not in self.tokens:\n",
    "            self.tokens.add(new_token)\n",
    "            new_id = len(self.token_to_id)\n",
    "            self.token_to_id[new_token] = new_id\n",
    "            self.id_to_token[new_id] = new_token\n",
    "\n",
    "        # 合併並更新詞彙表\n",
    "        query = ' '.join(pair)  # 替換的目標字元對 (有空白)\n",
    "        new_vocab = {}\n",
    "        for word, freq in self.vocab.items():\n",
    "            word_str = ' '.join(word) # 將原組資料轉換成字串 (有空白)\n",
    "            new_word_str = word_str.replace(query, new_token) # 用repalce移除目標字元對的空白\n",
    "            new_word = tuple(new_word_str.split()) # 通過空白切割字元並轉換成元組(以作為字典的鍵)\n",
    "            new_vocab[new_word] = freq\n",
    "        self.vocab = new_vocab\n",
    "\n",
    "    def bpe_iterate(self, num_merges):\n",
    "        for _ in range(num_merges):\n",
    "            pairs = self.get_stats()\n",
    "            if pairs:\n",
    "                best = max(pairs, key=pairs.get)\n",
    "                self.merge_vocab(best)\n",
    "        return self.tokens\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split()\n",
    "        tokenized = [] \n",
    "        for word in words:\n",
    "            word = tuple(word)\n",
    "\n",
    "            subwords = []\n",
    "            while word:  # 當word還有剩餘的字元時繼續迭代\n",
    "                for i in range(len(word), 0, -1):  # 從後面開始迭代，逐漸減少子詞的長度\n",
    "                    subword = ''.join(word[:i])\n",
    "\n",
    "                    if subword in self.tokens or i == 1:\n",
    "                        subwords.append(subword)  # 將子詞加入子詞列表中\n",
    "                        word = word[i:]  # 將已處理過的子詞從原單詞中移除\n",
    "                        break\n",
    "\n",
    "            tokenized.extend(subwords)  # 將處理完的子詞加入最終的tokenized列表中\n",
    "\n",
    "        # 將子詞轉換成對應的ID，如果子詞不在token_to_id中，則使用unk_token的ID\n",
    "        return [self.token_to_id.get(token, self.token_to_id[self.unk_token]) for token in tokenized]\n",
    "    \n",
    "    def pad_sequence(self, sequences, max_len=None, padding_value=0):\n",
    "        if max_len is None:  # 設定最大長度\n",
    "            max_len = max(len(seq) for seq in sequences)  # 若沒設定自動判斷\n",
    "        \n",
    "        padded_sequences = []\n",
    "        for seq in sequences:\n",
    "            # [原始文字] + [<PAD>] * 缺少的長度\n",
    "            padded_seq = seq + [padding_value] * (max_len - len(seq))\n",
    "            padded_sequences.append(padded_seq)\n",
    "        \n",
    "        return padded_sequences\n",
    "\n",
    "\n",
    "\n",
    "# 初始化詞彙表 (單字與其單字的出現次數)\n",
    "vocab = {'low': 5, 'lower': 2, 'newest': 6, 'widest': 3} # 表示low 這個單字在文檔中出現5次\n",
    "bpe = BPE(vocab)\n",
    "print('當前的Token:', bpe.tokens)\n",
    "print('當前的詞彙表:', bpe.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算相鄰字元出現頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字元對出現的頻率: Counter({('e', 's'): 9, ('s', 't'): 9, ('w', 'e'): 8, ('l', 'o'): 7, ('o', 'w'): 7, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3, ('e', 'r'): 2})\n",
      "出現次數最多的字元對 ('e', 's')\n"
     ]
    }
   ],
   "source": [
    "pairs = bpe.get_stats()\n",
    "best = max(pairs, key=pairs.get)\n",
    "print('字元對出現的頻率:', pairs)\n",
    "print('出現次數最多的字元對', best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合併出現次數最多的組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合併後的詞彙表: {('l', 'o', 'w'): 5, ('l', 'o', 'w', 'e', 'r'): 2, ('n', 'e', 'w', 'es', 't'): 6, ('w', 'i', 'd', 'es', 't'): 3}\n",
      "當前後的tokens: {'es', '<PAD>', '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "bpe.merge_vocab(best)\n",
    "print('合併後的詞彙表:', bpe.vocab)\n",
    "print('當前後的tokens:', bpe.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通過迭代計算Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最後的Token: {'new', 'est', '<PAD>', 'newest', 'wi', '<UNK>', 'widest', 'wid', 'lo', 'es', 'low', 'ne'}\n",
      "最後的詞彙表: {('low',): 5, ('low', 'e', 'r'): 2, ('newest',): 6, ('widest',): 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_merges = 9\n",
    "tokens = bpe.bpe_iterate(num_merges)\n",
    "print('最後的Token:', bpe.tokens)\n",
    "print('最後的詞彙表:', bpe.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用建立好的Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轉換後的Token_ids: [5, 3, 8, 11]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"lowest newest widest\"\n",
    "token_ids = bpe(test_text)\n",
    "print(\"轉換後的Token_ids:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Padding功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "填充前的結果: [[5, 3, 8, 11], [1, 1, 7, 1, 1, 1, 1, 1, 11]]\n",
      "填充後的結果: [[5, 3, 8, 11, 0, 0, 0, 0, 0], [1, 1, 7, 1, 1, 1, 1, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"lowest newest widest\", 'My new car is widest']\n",
    "token_ids = [bpe(test_text) for test_text in test_texts]\n",
    "print('填充前的結果:', token_ids)\n",
    "print('填充後的結果:', bpe.pad_sequence(token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
